{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Word + SVM\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset is Reuters-21578 dataset - news documents collected from the Reuter’s newswire in 1987. There are 7310 training documents, 3355 test documents. Each document contains text and labels. For example\n",
    "\n",
    "**Text:**\n",
    "\"chicago, march 11 - u.s. economic data this week could be the key in determining whether u.s. interest rate futures break out of a 3-1/2 month trading range, financial analysts said ...:\"\n",
    "\n",
    "**Labels:**\n",
    "castor seed,potato,barley,soybean,gas,crude,nickel,coconut,nkr,platinum,citrus pulp,yen,cotton,dfl,copper,fishmeal,dmk,hog,jobs,lead,rubber,interest,corn gluten feed,cruzado,inventories,grain,sugar,oat,ship,palm kernel,alum,reserves,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = 'running_result/tfidf-svm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-preprocess part. \n",
    "\n",
    "Construct documents as $[d_1,d_2,...,d_m]$ , where $d_i$ is string. \n",
    "\n",
    "Construct labels as $[y_1,y_2,...,y_m]$, where $y_i$ is variable-length list of string $[l_1,...,l_k]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Extract data from pickle files ==========\n"
     ]
    }
   ],
   "source": [
    "from src.data_preprocess.preprocess import DataSet\n",
    "dataset = DataSet()\n",
    "raw_x_train, raw_y_train, raw_x_test, raw_y_test = dataset.get_train_and_test_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert multilabel to single label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_single_raw = []\n",
    "y_train_single_raw = []\n",
    "\n",
    "class_df = {}\n",
    "\n",
    "for i, label_list in enumerate(raw_y_train):\n",
    "    for label in set(label_list):\n",
    "        x_train_single_raw.append(raw_x_train[i])\n",
    "        y_train_single_raw.append(label)\n",
    "\n",
    "for label in y_train_single_raw:\n",
    "    class_df[label] = class_df.get(label, 0) + 1\n",
    "\n",
    "x_train_single = []\n",
    "y_train = []\n",
    "\n",
    "for i, x in enumerate(x_train_single_raw):\n",
    "    if class_df[y_train_single_raw[i]] >= 5:\n",
    "        x_train_single.append(x)\n",
    "        y_train.append(y_train_single_raw[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8547, 7310, 8547)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_single), len(raw_x_train), len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text\n",
    "\n",
    "1. Tokenize document text into a list of tokens \\['u.s.', 'econom', 'data', 'debt', 'chicago', 'interest',...\\] using NLTK library and remove stop-words from tokens, such as \\['a', 'the', ...\\]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from src.data_preprocess.preprocess import stop_words_\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "stem = PorterStemmer()\n",
    "\n",
    "def convert_text_to_word_list(text: str) -> []:\n",
    "    word_tokens = word_tokenize(text)\n",
    "    tokens = []\n",
    "    for word in word_tokens:\n",
    "        # remove punctturation and stop word\n",
    "        word = word.replace('*', '')\n",
    "        if re.compile(r'[a-z]').search(word) is None or word in stop_words_ or len(word) <= 3:\n",
    "            continue\n",
    "        word = stem.stem(word)\n",
    "        if len(word) <= 3:\n",
    "            continue\n",
    "        tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_word_list = [convert_text_to_word_list(text) for text in x_train_single]\n",
    "x_test_word_list = [convert_text_to_word_list(text) for text in raw_x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Collect all tokens as a set of words - vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "for word_list in x_train_word_list:\n",
    "    words = words.union(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19120"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering.\n",
    "\n",
    "We use vocabulary V to build feature vector for each document.\n",
    "\n",
    "Vocabulary = {'econom', 'data', 'year', …}\n",
    "\n",
    "Document = \\['econom', 'interest', 'date', …\\]  -> \\[1, 0, 0, …\\] -- (|V|, )\n",
    "\n",
    "|   T      |  'econom'  |  'data' | 'year' | ... |\n",
    "|:--------:|:----------:|:-------:|:------:|:---:|\n",
    "| document |      1     |     0   |    0   | ... |\n",
    "\n",
    "One-hot method will convert each document to a binary vector. Combining with tf-idf as weight for each term, we can map text to a vector f ~ (|V|, ) and then convert list of documents \\[d1,d2,...,dm\\] to a matrix X ~ (m, |V|).\n",
    "\n",
    "1. calculate document frequency of each term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "for word_list in x_train_word_list:\n",
    "    for word in set(word_list):\n",
    "        df[word] = df.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. map word list to a vector using tf-idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf_idf(tf, df, doc_num):\n",
    "    \"\"\"\n",
    "    :param doc_num: the number of all documents\n",
    "    :param tf: term frequency\n",
    "    :param df: document frequency where term appears\n",
    "    :return: td-idf importance\n",
    "    \"\"\"\n",
    "    idf = np.log(float(doc_num + 1) / (df + 1))\n",
    "    tf = 1.0 + np.log(tf)\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = len(x_train_word_list)\n",
    "\n",
    "def map_word_list_to_tfidf_vector(word_list, words):\n",
    "    vocabulary = {}\n",
    "    for i, word in enumerate(words):\n",
    "        vocabulary[word] = i\n",
    "    \n",
    "    x = np.zeros(len(vocabulary))\n",
    "    tf = {}\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word in vocabulary.keys():\n",
    "            tf[word] = tf.get(word, 0) + 1\n",
    "    \n",
    "    for word in tf.keys():\n",
    "        j = vocabulary[word]\n",
    "        x[j] = calculate_tf_idf(tf[word], df[word], n)\n",
    "\n",
    "    return x.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [map_word_list_to_tfidf_vector(word_list, words) for word_list in x_train_word_list]\n",
    "x_test = [map_word_list_to_tfidf_vector(word_list, words) for word_list in x_test_word_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Input is X(m, |V|), labels is Y(m,). Use SVM to solve the classification problem.\n",
    "\n",
    "## Model Selection\n",
    "\n",
    "We use K-fold cross validation to select best model between different models which may have different parameters and different kernels (linear, RBF). For example\n",
    "\n",
    "```python\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100]}, {'kernel': ['linear'], 'C': [1, 10, 100]}]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4, 1e-5], 'C': [1, 10, 100]}, {'kernel': ['linear'], 'C': [1, 10, 100]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 120.9min\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed: 121.9min\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed: 123.5min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed: 183.9min\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 187.6min\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed: 188.8min\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed: 189.0min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed: 189.7min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 237.2min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 239.9min\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed: 239.9min\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed: 298.2min\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed: 303.5min\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed: 303.7min\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed: 304.6min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed: 368.6min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 411.5min\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed: 413.1min\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed: 414.7min\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed: 415.7min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed: 418.5min\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed: 420.1min\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed: 474.5min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 476.6min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed: 520.2min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed: 522.7min\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed: 523.2min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed: 525.4min\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed: 527.2min\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed: 528.3min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "# train svm via cross validation\n",
    "A = time.time()\n",
    "clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='precision_weighted', verbose=15, n_jobs=-1)\n",
    "clf.fit(x_train, y_train)\n",
    "print('Cross validation time: {}'.format(time.time() - A))\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)\n",
    "\n",
    "cv_result = pd.DataFrame.from_dict(clf.cv_results_)\n",
    "with open('{}/cv_result.csv'.format(result_path),'w') as f:\n",
    "    cv_result.to_csv(f)\n",
    "\n",
    "# save model\n",
    "joblib.dump(clf, '{}/cv_model.joblib'.format(result_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we found the best parameters as following:\n",
    "\n",
    "```python\n",
    "Best parameters set found on development set:\n",
    "{'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
    "0.374 (+/-0.046) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
    "0.193 (+/-0.024) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
    "0.446 (+/-0.053) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
    "0.437 (+/-0.073) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
    "0.424 (+/-0.059) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
    "0.447 (+/-0.035) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
    "0.409 (+/-0.036) for {'C': 1, 'kernel': 'linear'}\n",
    "0.409 (+/-0.038) for {'C': 10, 'kernel': 'linear'}\n",
    "0.409 (+/-0.038) for {'C': 100, 'kernel': 'linear'}\n",
    "```\n",
    "\n",
    "We use the accuracy as the performance metric of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_predict_accuracy(y_pred, y_test):\n",
    "    # test\n",
    "    sum_acc = 0\n",
    "    for j, pre in enumerate(y_pred):\n",
    "        if pre in y_test[j]:\n",
    "           sum_acc += 1\n",
    "\n",
    "    return sum_acc / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"Accuracy: {}\".format(cal_predict_accuracy(y_pred=y_pred, y_test=raw_y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of feature selection\n",
    "\n",
    "The vocabulary size is about 20000. It is useful to sort the vocabulary by the importance of each word which can be calculated by total term frequency, chi square and tf-idf for class, because the vocabulary size N equals the dimension of document feature vector and larger N will cause high time complexity, so we can select top N words instead of selecting all terms for reducing time complexity of training.\n",
    "\n",
    "Select tf-idf as importance metric for terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "    \n",
    "def calculate_priority_by_tfidf(X: [], y: []) -> {}:\n",
    "    term_df = {}\n",
    "    term_tf = {}\n",
    "    tf = {}\n",
    "    min_count = 4\n",
    "    _n = len(X)\n",
    "    i = 0\n",
    "    \n",
    "    for word_list in tqdm(X):\n",
    "        terms = set(word_list)\n",
    "        for term in terms:\n",
    "            term_df[term] = term_df.get(term, 0) + 1\n",
    "\n",
    "        class_ = y[i]\n",
    "        i += 1\n",
    "        if class_ not in term_tf.keys():\n",
    "            term_tf[class_] = {}\n",
    "\n",
    "        for term in word_list:\n",
    "            term_tf[class_][term] = term_tf[class_].get(term, 0) + 1\n",
    "\n",
    "    for class_ in term_tf.keys():\n",
    "        for term, v in term_tf[class_].items():\n",
    "            if v >= min_count:\n",
    "                tf[term] = tf.get(term, 0) + v\n",
    "\n",
    "    term_importance_pair = {}\n",
    "    \n",
    "    for term in tf.keys():\n",
    "        term_importance_pair[term] = calculate_tf_idf(tf[term], term_df[term], _n)\n",
    "\n",
    "    return term_importance_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bag_of_words(calculate_priority=calculate_priority_by_tfidf) -> []:\n",
    "    n_doc = len(x_train_word_list)\n",
    "\n",
    "    #print(\"========== Feature selection ==========\")\n",
    "    term_importance_pair = calculate_priority(x_train_word_list, y_train)\n",
    "    temp = []\n",
    "    for term, value in term_importance_pair.items():\n",
    "        temp.append([term, value])\n",
    "\n",
    "    #print(\"Sort terms by importance...\")\n",
    "    temp = sorted(temp, key=lambda pair: pair[1], reverse=True)\n",
    "    vocabulary = [item[0] for item in temp]\n",
    "\n",
    "    if len(vocabulary) == 0:\n",
    "        print(\"Warning! After pruning, no terms remain. Try a lower min_df or a higher max_df.\")\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocabulary = generate_bag_of_words(calculate_priority_by_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the best parameters from the cross validation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='rbf', gamma=0.0001, C=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "accuracy = []\n",
    "vocab_size_step = [int(i) for i in np.logspace(start=np.log10(100), stop=np.log10(len(vocabulary)), num=10, base=10)]\n",
    "\n",
    "for vocab_size in vocab_size_step:\n",
    "    A = time.time()\n",
    "    tmp_vocab = sorted_vocabulary[0:vocab_size]\n",
    "    print('vocabulary_size: {}'.format(vocab_size))\n",
    "\n",
    "    x_train_tmp = [map_word_list_to_tfidf_vector(word_list, tmp_vocab) for word_list in x_train_word_list]\n",
    "    x_test_tmp = [map_word_list_to_tfidf_vector(word_list, tmp_vocab) for word_list in x_test_word_list]\n",
    "    \n",
    "    svm_model.fit(x_train_tmp, y_train)\n",
    "    y_pred = svm_model.predict(x_test_tmp)\n",
    "    acc = cal_predict_accuracy(y_pred=y_pred, y_test=raw_y_test)\n",
    "    print(\"Accuracy: {}, Time: {}\".format(acc, time.time() - A))\n",
    "    accuracy.append(acc)\n",
    "\n",
    "plt.plot(vocab_size_step, accuracy, marker='x')\n",
    "plt.xlabel('vocabulary_size')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('classification accuracy with respect to different vocabulary_size')\n",
    "plt.savefig('{}/acc-ver-vocab-size.svg'.format(result_path))\n",
    "with open('{}/acc-vocab-size.txt'.format(result_path), 'w') as f:\n",
    "    f.writelines('{}'.format(vocab_size_step))\n",
    "    f.writelines('{}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the best result is 0.89 accuracy.However, when vocabulary size is 8000, the accuracy (0.82) is good enough which means feature selection can benefit on reducing time complexity as well as keep performance of classification.\n",
    "\n",
    "![](https://i.loli.net/2018/12/29/5c2708e4d3b05.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
